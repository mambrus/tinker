\part{Writing TinKer - the sleepy kernel}
\chapter{Introduction}
A kernel can be written in numerous ways. TinKer's does not in any way claim that this the only way, neither the correct way to do it. Only that this is one way of doing it, based on a certain set of philosophies, ambitions and ideas. One of them those ambitions is to make the kernel as readable and comprehensive as possible.

One of the original intentions with TinKer from the very start, was to write a kernel that would exemplify a text about kernels (this text actually). Therefore you will hopefully find that TinKer is written in such a way that you will be able to follow the code. Kernel theory is a quite difficult subject not normally taught to computer science novices. My ambition is also to make this knowledge available the fresh mind of a student, on so that he'll be able to approach his upcoming carrier with a toolbox of knowledge. thereby making him able to select better solutions for his projects and to identify the really bad ones (most oftenly those based on FUD and who takes advantage of peoples uncertainty and ignorance),

You will notice that this text does not contain many references. This is intentional for two reasons. The first reason is that most of the text here is based on my own insight and therefore there is no natural references. I.e. by doing this I'm also vulnerable for that some of the conclusions are wrong- I'm delighted however to notice that many of the conclusions are verifiable, and should I detect something thats inaccurate I will naturally change those conclusions.  The second reason is a little more difficult to realize. This subject is very difficult. Most oftenly the difficulty lays in the fact that it's difficult to explain. It's like math, you need to come to "insights". The formulas and what-not are just notation (or language). One has to learn the notation to be able to come to insight\ldots.

When it comes to computer science there are many notations and it spawns from computer languages, to graphical descriptive languages and mathematical expression. For the advanced scientist, using any of these (based on the subject of course) this is naturally the best way. But for the novice, this becomes a threshold most oftenly very hard to overcome in a limited amount of time. Scientific work is normally written in such notation however.

\begin{table}[!hbp]
\begin{tabular}{|r|rl|}
\hline
CPU architecture 	& Tool-chain 	& execution type\\ \hline
x86 			& $GNU_1$	& Linux \\
x86 			& $GNU_1$	& Cygwin/Win32 \\
x86 			& MSVC		& Win32 \\
x86 			& Borland	& Win32/DOS \\
ARM7			& $GNU_3$	& HW\\
ARM7			& $GNU_2$	& HW\\
Blackfin		& $GNU_2$	& HW\\
PowerPC 		& $GNU_2$	& HW\\
C166 			& Keil		& HW\\
8051 			& IAR 		& HW (old discont.)\\
Z80 			& IAR 		& HW (old discont.)\\ \hline
\end{tabular}

\caption{TinKer ports (Nov 2006)}\label{ports}
\end{table}

\marginpar{%
$GNU_1$ glibc\\ $GNU_2$ newlib / HIXS\\ $GNU_3$ newlib / def
}


%------------------------------------------------------------------------------
%------------------------------------------------------------------------------
%------------------------------------------------------------------------------

\chapter{Quick tour - writing a kernel in 30 minutes}
\section{Genesis}
\label{kernel30}
I assume that you have allready written at least one simple program. What is that when you look upon it really? What \textit{defines} a program?

\begin{itemize}
\item A program is an entity that describes or instructs another entity what to do.
\item Doing so by implying a certain set of rules
\end{itemize}

	\paragraph{Sequence}

		In a computer program this boils down to something very similar to a written piece of paper. This might sound very obvious, but when you think of it. to be able to make any sense of what is written, we have to imply a certain set of rules. Normally you'd expect interpretation to be \textbf{sequential} - i.e. one thing has to come before another in a certain order. Another thing is to define this \textbf{order} of execution in terms of left to right, top to bottom e.t.a.
\\
		A computer program is actually very similar. It has a defined order (normally from top to bottom) and it is normall thought of as sequential flow of execution. Think of it as a puncard, with a lot of holes placed in certain places on a piece of carbon paper (this is actually more than an analogy, it happens to be a historical fact of the very early computer memories).
\\
		We'll keep this analogy in mind for a while, even though it is not strictly true in all aspects.
	\begin{figure}[!hbp]
	\begin{dotpic}
		node [
			shape=record,
			style=filled,
			fillcolor=yellow,
			fontname=Courier,
			nojustify="true",
			fontsize=10.0
		];

		edge [
			color="red",
			fontname=Courier,
			nojustify="true",
			fontsize=10.0
		];

		graph [
			rankdir = "TB",
			fontname=Courier,
			nojustify="true",
			fontsize=10.0
		];

		MAIN [ orientation=73.0, label="{\
			---- START ----         | \
			do something   | \
			<m1> jump if somewhere  | \
			<m2> jump if somewhere   | \
			<m3> do something   | \
			<m4> jump if somewhere   | \
			<m5> do something   | \
			<call_from> call something | \
			<ret_from_call> continue do something | \
			---- END ---- }"];

		PROC [ orientation=73.0, label="{\
			<pi>  ---- PROC START ---- | \
			do something   | \
			do something   | \
			do something   | \      
			<po>  ----PROC END ----}"];


		MAIN:call_from:e -> PROC:pi:e 
		PROC:po:w -> MAIN:ret_from_call:w [label=RET];
	\end{dotpic}
	\caption{Flow of execution.\label{execflow}}	
	\end{figure}

	In the graph above, we've exemplified the analogy but also introduced yet two more common properties in a computer program, that of \textbf{execution control} (jump, if, while, case e.t.a.) and that of \textbf{encapsulation and reuse} (proc, function e.t.a.). These properties are not common in normal written languages but there exists other notations. Musical notes are a good example.
\\
	The latter two properties are really not important for our reasoning, but what is important is the idea of of an execution sequence and it's order. The red arrows are attempting to lead you into this thinking. You should now feel confortable with the expression \textit{"thread of execution"} and an analogy of the directed sequence or flow. \textit{"Thread of execution"} can sometimes be shortened with just the word \textit{"thread"}.

	\paragraph{thread}
	\label{genesis_thread}
		Once one has accepted the analogy of \textit{"thread of execution"} one needs to add yet another fundamental property until we can call it a \textit{computer thread}.

		If \textit{"thread of execution"} symbolised the whole thread of execution in a processor, a program thread is thought of a tiny part of this total thread. Now, it's important to realize that it is a part of that totality, and hence shares that "big" threads inbound notion of sequence and order. It's really simple, just think of it as a sawing thread that you've cut in a couple of pieces.

		Notice how we're speaking of \textit{one} thread of execution. In a single core CPU there can only exist one thread of execution. I.e. if that CPU runs with a kernel or OS, all these executives are just seemingly parallel. Sometimes the word concurrent programing is used to point out this fact. Only multicore CPU's or distributed systems can have \textit{true} parallelism. Even though concurrency is  a more accurate term than parallel, in this text we'll use both of them interchangeably.

		Speaking of which, what is the difference between a thread and a program anyway?
		Answer: there is none (it only appears that way).

		Assume that somewhere there exists four functions called  do\_1, do\_2, do\_3 and do\_4 as in the program in table~\ref{hc1}.

		\begin{table}[!hbp]
		\begin{verbatim}
		#include "dofuncs.h"
		#define FOREVER 1
		int main( int argc, char **argv)
		{ 
        		while( FOREVER )
        		{
		                do_1();
		                do_2();
		                do_3();
		                do_4();
		        }
		}
		\end{verbatim}
		\caption{Hard-coded shedule.\label{hc1}}
		\end{table}

	You will have to take my word for it for now. But those function calls in the above example are very similar (almost identical as a matter of fact) to threads.
	\\\\
	Imagine circles in in the diagrams that follow (figure~\ref{simpl1} and figure~\ref{simpl2}) representing functions, the arrows representing calls \& returns and the adjacent number the execution order.

	%\clearpage
	\begin{figure}[!hbp]
	\begin{dotpic}
		node [shape=circle,fontsize=10.0];
		edge [fontsize=10.0];
		graph [rankdir = "TB",fontsize=10.0];

		do_1
		do_2
		func	
		do_3
		do_4

		func -> do_1		[label="1"]
		do_1 -> func		[label="2"]
		func -> do_2		[label="3"]
		do_2 -> func		[label="4"]
		func -> do_3		[label="5"]
		do_3 -> func		[label="6"]
		func -> do_4		[label="7"]
		do_4 -> func		[label="8"]
	\end{dotpic}
	\caption{Call-diagram of simple program.\label{simpl1}}	
	\end{figure}
	\footnote{The  middle circle in figure~\ref{simpl1} is the loop in our program in table~\ref{hc1}}
	\begin{figure}[!hbp]
	\begin{dotpic}
		node [shape=circle,fontsize=10.0];
		edge [fontsize=10.0];
		graph [rankdir = "TB",fontsize=10.0];

		do_1
		do_2
		func	
		do_3
		do_4

		do_1 -> func		[label="1"]
		func -> do_2		[label="2"]
		do_2 -> func		[label="3"]
		func -> do_3		[label="4"]
		do_3 -> func		[label="5"]
		func -> do_4		[label="6"]
		do_4 -> func		[label="7"]
	\end{dotpic}
	\caption{Call-diagram of "something else".\label{simpl2}}
	\end{figure}
		
	Now, what is the difference between the graph in figure~\ref{simpl1} and that of figure~\ref{simpl2}?.

	Nothing really. The only difference is that the bubble called do\_1 starts the whole sequence, and it calls the middle circle instead of the other way around. \textit{From that point and onward, they are exactly the same.} Yet, this is conceptually speaking really all that differs between a a kernel with it's threads and any other normal program\ldots Imagine if you could have that while-loop, but just start the whole thing from within the first thread (or any of them for that matter).

	The rest of this text aims you convince you that these two graphs \textit{are in fact identical}, and to show how the wish can come true.

\section{The dispather}
	In reality, what differs between the first and second graph is a entity called the \textbf{dispatcher}. A dispatcher does pretty much what the program above does, except that it usually doesn't work with statically linked functions. Instead it uses \textbf{function pointers}. Have a look at the program in table~\ref{hc2}.

	\begin{table}[!hbp]
	\begin{verbatim}
	#include "dofuncs.h"
	#define FOREVER 1
	typedef void *func_poiner();

	func_poiner fu_1;
	func_poiner fu_2;
	func_poiner fu_3;
	func_poiner fu_4;

	void dispatcher()
	{
	        while( FOREVER )
	        {
	                fu_1();
	                fu_2();
	                fu_3();
	                fu_4();
	        }
	}

	int main( int argc, char **argv)
	{
	        fu_1 = do_1;
	        fu_1 = do_2;
	        fu_1 = do_3;
	        fu_1 = do_4;
	}
	\end{verbatim}
	\caption{Soft-core shedule.\label{hc2}}
	\end{table}

	A function pointer is nothing but a variable that keeps the address of a function. Normally with the intention to also call those function, just as if they were any other normal statically linked functions. Many computer languages has similar concepts, but "C" is particularly easy to program concerning function pointers. (BTW, this happens to be what object methods are In C++ - but that is a totally different story).
\\
	The program in table~\ref{hc2} actually defines a very simplified kernel. Only a few conceptual details are missing really. We'll cover them in later chapters. Until then, do please enter~\ref{hc2} in you computer and convince yourself that it really works.

\section{The schedule}
\subsection{A "job list"}
	What would happen if we instead of having separate variables for the functions were to put those in and array like in the picture~\ref{FunTable}?
	\begin{figure}[!hbp]
	\begin{dotpic}
		node [shape=record,fontsize=10.0,style=filled,fillcolor=yellow];
		edge [fontsize=10.0];
		graph [rankdir = "TB",fontsize=10.0];
		FunTable[label="do_1 | do_2 | do_3 | do_4"]

	\end{dotpic}
	\caption{Array of function tables.\label{FunTable}}	
	\end{figure}
	Then just imagine, we could change the order of their execution just by moving them around. Or we could increase one threads frequency by entering it several times in such an array\ldots We could even make the array larger than needed and put some kind of a stopper at the end\footnote{These entities are commonly known as \textit{"sentinels"}}. That we could alter that list whenever needed in run-time\ldots
	\begin{figure}[!hbp]
	\begin{dotpic}
		node [shape=record,fontsize=10.0,style=filled,fillcolor=yellow];
		edge [fontsize=10.0];
		graph [rankdir = "TB",fontsize=10.0];
		FunTable[label="do_3 | do_1 | do_2 | do_1 | do_4 | do_1 | \<NULL\>"]
	\end{dotpic}
	\caption{Reordered array. Notice that do\_1 occurs 3 times.\label{FunTable2}}	
	\end{figure}

	\begin{table}[!hbp]
	\begin{verbatim}
	#include <stdio.h>
	#define FOREVER 1
	#define NUMTHREADS 100
	typedef void *func_poiner();
	func_poiner furray[NUMTHREADS];

	void do_1(){printf("Run 1\n");}
	void do_2(){printf("Run 2\n");}
	void do_3(){printf("Run 3\n");}
	void do_4(){printf("Run 4\n");}

	void dispatch()
	{
	        While (FOREVER)
	                for(i=0; furray(i); i++ );
	}

	int main( int argc, char **argv)
	{
	        furray[0] = do_3;
	        furray[1] = do_1;
	        furray[2] = do_2;
	        furray[3] = do_1;
	        furray[4] = do_4;
	        furray[5] = do_1;

		dispatch();
	}
	\end{verbatim}
	\caption{Scheduled execution.\label{schedued1}}
	\end{table}
	
	If you want, you can try to modify the program and let one of the threads modify the schedule. Notice though that the thread\footnote{Our \textit{"dispatcher"} requires the \textit{"threads"} to be non-closed. I.e. they have to finish in one run} has to be open\footnote{This is still the case even in TinKer, but since we have multiple entry and exit points in TinKer, it's doesn't have to be from \textit{"start"} to \textit{"end"}.}.

\subsection{Some \textit{jobs} are more important than others}
A very common issue in control systems is the need to make a difference between the importance of certain activities. For example, interacting with the user can be considered less important than acting on a certain certain event. The less important activity can then be postponed or held back for a while until the important job has been completed.
	
	\begin{figure}[!hbp]
	\begin{dotpic}
		graph [rankdir = "LR",fontsize=10.0];		
		style=filled;
		color=white;
		node [style=filled, color=white, fontcolor=white];
		N0 -> N1 [label="priority"]
	\end{dotpic}
\\
	\begin{dotpic}
		graph [rankdir = "TB",fontsize=10.0];
		subgraph cluster_0 {
			style=filled;
			color=white;
			node [style=filled, color=white, fontcolor=white];
			N0 -> N1 [label="order"]
		}
		node [shape=record,fontsize=10.0,style=filled,fillcolor=yellow];
		edge [fontsize=10.0];
		graph [rankdir = "TB",fontsize=10.0];
		FunTable[label="\
			{ T3    |   T1  |   T7  | \<0\> |       |       } | \
			{ T2    |  T5   | \<0\> |       |       |       } | \
			{ T4    |  T6   |       | T8    | T9    | \<0\> } \
		"]
	
	\end{dotpic}
	\caption{"A two dimensional schedule. The 2:nd dimesion is our \textit{"priority"}.\label{FunTable3}}	
	\end{figure}


\begin{table}[!hbp]
\begin{tabular}{|l|c|c|c|c|c|}
\hline
		& Mon 	& Tue	& Wed	& Thu	& Fri\\ \hline
 8:15 -  9:00 	& math	&	& Phys	& prog	& play\\
 9:00 -  9:45 	& math	& music	& Phys	& prog	& play\\
10:00 - 10:15 	& hist	& wood	& math	& comp.sc	& electronics\\
10:15 - 11:30 	& hist	& wood	& math	& comp.sc	& electronics\\ \hline
11:30 - 12:15 	& LUNCH	& LUNCH	& LUNCH	& LUNCH	& LUNCH\\ \hline
12:15 - 13:30 	& Eng	& relig	& biol	& C++	& mechanics\\
13:30 - 14:15 	& Eng	& relig	& bio	& C++	& mechanics\\
14:15 - 15:30 	& art	& music	& chem	& Java	& rocket.sc\\
15:30 - 16:15 	& art	&	& chem	& Java	& rocket.sc\\ \hline
\end{tabular}
\caption{A school schedule inspired TinKer's schedule}\label{school_schedule}
\end{table}


The program in table~\ref{schedued2} shows you a simple example of how to implement and run this prioritised schedule. We're closing in on how Tinker's schedule looks like. 

Tinker's schedule follows the basic outline of our schedule so far very closely. However we can't use function pointers at each cell. If our kernel were to be running threads with a single point of entry and a single point of exit, then we could. But since we want our kernel to be able to do some other nice stuff\footnote{\ldots{}like sending and receiving information between threads using queues, semaphores e.t.a.} we need to put something that holds all the information needed to do that. We need to put each threads \textit{envelope} there instead. We call it the \textit{thread control block} or \textit{TCB}.

Yet one more concept is missing. Not everything about the kernels behaviour can be kept in the schedule. We need to apply certain rules of how to interpret it also. The understanding and implementation of these rules are the job of the dispatcher. For example, the fact that the schedule is now two dimensional and that the second dimension is to be interpreted as the priority is such a \textit{"rule"}. 

How to implement such a rule can in turn be dictated by other rules. For example, dispatch threads at the same priority level in round-robin to give them equal chance to run. Handling of cases to avoid priority inversion could be another rule\footnote{Priority inversion can be avoided by temporary rise the priority of a thread to the same level as the highest level of a thread blocked on it. These advanced dispatching principles has not been addressed in TinKer yet.} 

TinKer has only a few of these rules, and they are hard coded in the logic of the dispatcher. In the literature, one sometimes find expressions like \textit{scheduling techniques} and \textit{scheduling principles} used a little carelessly. I would like to point out that there is a difference between the rules of scheduling and the rules of dispatching. Many times when people talk about scheduling principles, they real mean dispatching principles. Using the correct terminology makes it a easier to know which part of the code in TinKer we're addressing\ldots

As a rule of thumb, everything involved with \underline{reading} the schedule and \textit{acting} based upon that added with additional logic is belongs to the \textit{dispatching domain}. Anything involved with permanently \textit{changing} the schedule belongs to the \textit{scheduling domain}.


	\begin{table}[!hbp]
	\begin{verbatim}
	#include <stdio.h>
	#define FOREVER 1
	#define THREADS_IN_PRIO 10
	#define NUMPRIO 3
	typedef void *func_poiner();
	func_poiner purray[NUMPRIO][THREADS_IN_PRIO];

	void T1(){printf("Run 1\n");}
	void T2(){printf("Run 2\n");}
	void T3(){printf("Run 3\n");}
	void T4(){printf("Run 4\n");}
	void T5(){printf("Run 5\n");}
	void T6(){printf("Run 6\n");}
	void T7(){printf("Run 7\n");}
	void T8(){printf("Run 8\n");}
	void T9(){printf("Run 9\n");}
	void NUKED(){}

	void dispatch()
	{
	        While (FOREVER)
	                for(prio=0;prio<NUMPRIO; prio++)
	                	for(i=0; purray(i); i++ );
	}

	int main( int argc, char **argv)
	{
	        purray[0] = {T3,T2,T7};
	        purray[1] = {T2,T5};
	        purray[2] = {T4,T6,NUKED,T8,T9};

		dispatch();
	}
	\end{verbatim}
	\caption{Prioritised schedule.\label{schedued2}}
	\end{table}
	This concludes the basic ideas that's the foundation of TinKer\ldots

\section{Yielding}
	The operation most associated with dispatching in TinKer is the \textit{yield()} function as shown in table~\ref{yield}. Conceptually yield means \textit{telling the kernel that it might switch} me\footnote{I.e. the current thread} out of context and switch someone else instead if needed.
	\begin{table}[!hbp]
	\begin{verbatim}

	void tk_yield( void ){
	   TK_CLI();   
	   PUSHALL();   
	   TK_STI();

	   _tk_wakeup_timedout_threads();
   
	   TK_CLI();
   
	   //Do not premit interrupts between the following two. Proc statuses 
	   //(i.e. thread statuses) frozen in time.
	   thread_to_run = _tk_next_runable_thread();
	   _tk_context_switch_to_thread(
	      thread_to_run,active_thread
	   );   
	   POPALL();
	   TK_STI();
	}
	\end{verbatim}
	\caption{TinKer yield implementation.\label{yield}}
	\end{table}
	The yield function is called by all TinKer public API as a \textit{"side effect"} right before each function exits. The implementation of yield might differ depending on how the kernel is compiled. This example is showing yield as if compiled default. 

	Yield is what makes concurrency work in a non-preemptive environment. It can be seen a sort of intended interrupt, but one thats initiated by the executing thread\footnote{Which is not how an interrupt works}.

	The analogy is half bad, because two of the interrupts major properties are not there\footnote{Asynchronousity and externl initiative}. But the part that's conceptually equal is that when enter the function\footnote{The ISR is called} and if the system detects that our call (i.e. our imaginary interrupt) is a valid event, a certain thread\footnote{A certain routine will be performed accordingly} will be started. 

	If there is nothing to be done, the program \footnote{I.e. our thread} will just continue normally. And this is also the situation when our imaginary interrupt has completed.

	Using yield in your program is not natural, and normally you wouldn't do that. Remember, yield is executed for you and unless you have really long paths of execution between entry/exit point's you don't have to call it.

	In principle, the same technique could be extended to cove each \textit{"C"} line in your code. This would be a kernel that has all the properties of a preemptive kernel, but without the drawbacks associated with stability and nasty bug crashes. This has to my knowledge never been done, the reason is the ratio between application code kernel which would lead to really poor performance.

	As I already mentioned, yield can look different depending on how the kernel is compiled\footnote{This is handled by conditional compilation directives (\#ifdef)}  the version in table~\ref{yield}. The issue is whether to use exclusive preemption\footnote{Build option '--enable-dispatch=EXCLUSIVE'} or not. If kernel is built that way, there is no need for yielding and the public yield will be replaced by a NULL pointer\footnote{I.e. any calls to yield will be skipped}.

	Instead dispatching is expected to take place at the certain event's affecting our system. I.e. external interrupts or timer timeouts. 

	Please note, that your program can still look the same and you don't need to change anything. In principle you should be able to recompile and run yore application in one \textit{"mode"} or the other without any modifications\footnote{This assumes you're code is correct and doesn't rely on \textit{"side effects"} of any of the \textit{"modes"}}.
\\\\
	Previously I said that TinKer was very much influenced by the simple programs so far in this chapter. Yet, our yield function doesn't look anything like a while loop does it? 

	It's true the function doesn't have a closed loop \textit{inside}, but it's still there. The loop is extended outside, and if you investigate the idle thread\footnote{A TinKer bases application usually has a couple of system threads. Two which are always present are \textit{"idle"} and \textit{"root"}.}, you'll find the loop there instead. See table~\ref{idle}
	\begin{table}[!hbp]
	\begin{verbatim}
	void *_tk_idle( void *foo ){
 
	   while (TRUE){
	      tk_yield();
	   }
	}
	\end{verbatim}
	\caption{TinKer idle thread.\label{idle}}
	\end{table}
	
	It looks like the loop is just extended one level. But do not missinterpret this by thinking context switching can only occur here. This is just where it occurs if all other threads are blocked. All threads are expected to interact with the kernel regularly by using it's API, and by doing this they will also pass entry/exit point's regularly where yielding will also be done.

	I.e. yielding can occur at any levels and any priorities. It doesn't impose any restrictions on the caller, i.e. any thread can call it at any time. Whether \textit{if} any action takes place or not is not for the caller to decide, only \textit{when}\footnote{I.e. at the point where the entry/exit point (or yield) is located.} it might do so.
	

\section{Who's the scheduler?}
\textit{This section is an discussion about run-time vs. compile-time scheduling. It's intended for the advanced reader.}
\\\\
The process of "scheduling" is always a matter for the programmer in way or the other. This can either be in the form as the first examples in this chapter, or more or less aided by the computer system the program will run on. The former is known as hard-coded schedules and the second as soft schedules. 

TinKer is a soft schedule based kernel, which is the case for the majority of OS's and kernels, even among those claiming to be addressing temporal space. 

No matter with type of scheduling technique your computer target provides, human intelligence is always at the top. Scheduling should at the very least be regarded as a combine effort by the programmer and the kernel. I.e. for a soft schedule based kernel, the programmer needs to know which API\footnote{API that will change the schedule are those involved in \textit{creation}, \textit{destruction} and \textit{priority changing} of a thread. A few implicit situations will also change the schedule, like exiting a thread\ldots} will change the schedule.

\marginpar{%
\it{We'll use the terminology hard schedule for hard-coded schedule and soft-schedule for run-time modifiable schedule. This might be the origin of the terms \textit{hard-} and \textit{soft real-time}, which is an unfortunate usage of missleading terminology. This text will avoid using the terms hard/soft real-time.}
}

When the programmer \textit{"schedules"} a system design, he usually would not use the same analogy\footnote{in the form of a "spread sheet" or "school schedule"\textit{}} as TinKer does. He'll probably use some formal methodology that eventually renders in a list of certain tasks and priorities. In some cases also perhaps a additional list of a sequence of changes to these priorities according to certain real world events 

When this program later executes, Tinker's internal schedule will be affected in run-time accordingly and will reflect the programmers design in TinKer schedule form. I.e. Tinker's schedule will be set when the program runs. 

The reason I mention this is that the process of scheduling can also be defined in compile-time. Some scholars in the area of real-time in the temporal domain prefer, that method because even the slightest change in the schedule takes a certain amount of time. In a fully temporal deterministic system you need to have control over \textit{ALL}\footnote{Soft scheduling in run-time might be deemed unacceptable in certain cases.} aspects of time, including those modifying the schedule.

Whether this is good or bad, true or false is a subject of discussion that's been going on for decades.

I do agree with the fact that soft scheduling adds complexity that makes temporal analysis more vulnerable and less trust-worthy. But I would also like to point out the fact that this price for using hard coded scheduling techniques is high and it's just not well suited for the majority systems.\footnote{There are some exceptions. For example control system where extreme safety is required. Extreme in this case, is a justified estimation between cost of lives and cost of system} Don't make sloppy decisions regarding this matter, thinking "better safe than sorry"\footnote{Extreme case where hard-coded technology is chosen} or "let's wing it"\footnote{The other extreme case where soft a soft schedule is used}. If you don't know what your doing please \textit{RUN}\footnote{Both these cases are disastrous if decisions are based on the wrong or factsassumptions.}.
\\\\
TinKer provides a reasonable compromise and can address the needs of the temporal domain because $~{a)}$ the time involved in modifying the schedule is in most cases neglectable and in practice it $~{b)}$ either occur seldomly or in a very distinct patterns\footnote{The master-worker programming model is an example where the schedule will be heavily modified in a deductable way.}. Furthermore you can $~{c)}$ read the code yourelf and deduct the times each schedule modifying operation takes. TinKer is also $~{d)}$ really not that complex, and it's not an impossible or impractical task to determine the execution flows involved with schedule modifications.
\\\\
What you should \textit{never} do if addressing the truly temporal domain though, is relaying on \textit{measurements} and \textit{statistical} distribution. In that case you have left any aspects of real-time long behind you, since you're dependant of that ever so small chance that the unknown will happen. I.e. you're breaking the first principle of real-time - determinism.


%------------------------------------------------------------------------------
%------------------------------------------------------------------------------
%------------------------------------------------------------------------------
\chapter{Fundamentals}

	\section{Context and context switching}
		Before we go any further, we need to mention what is meant by a \textit{context} and what a \textit{context switch} is.
	
		\subsection{Context}
\marginpar{%
\it{Contex $\in$ Environment}
}
			The context you could say is the processors \textit{"environment"} - or more accurately, it's not a property belonging to the processor but to the \textit{executive} (or \textit{execution entity}), which in a system without neither kernel nor OS can be regarded as the same thing. 

			It is the memory, certain states it has achieved and it's registers. The memory in turn can be divided in several part's. From a thread's point of view only one kind of memory matters and that is the part constituting it's stack. For fully fledged OS's more than the stack would belong to each executing environment, it would actually have it's own set of all types of memory.
	
		\subsection{Context switching}
			\textit{Context switching} is a just what the words say: Switching one environment with another. In reality this is a little bit more complicated, but not much. Our context is the stack belonging to the thread. Almost everything we need to \textit{switch} is allready there. We only need to store the CPU's internal registers on top of that stack and we're prepared for the switching. The switching itself is a very simple operation of getting the value of the CPU's stack-pointer (that will point to TOS\footnote{In this text TOS is an acronym for Top Of Stack, i.e. the region of the stack close to or at the top}), store that value away in a safe place. The the new theads stack pointer stored on another safe place and enter that value in the CPU stack-pointer itself.

			If that new stack is properly prepared, the rest will take care of itself. The registers will be restored with the ones previously saved on that stack, and when the execution will also return to the new address stored in the new stack.
			

	\section{Processes vs threads}
		Both processes and threads are execution entities. They are conceptionally very similar, almost identical.\\
		\\
		Not to long ago I heard something funny. The Linux community had just received it's first implementation of a working POSIX 1003.1c compliant kernel (i.e. kernel threads). The whole Linux community was praising this great innovation when some wise guy responded that \textit{"the embedded community has been able to do just that for more than 20 years"}\ldots

		Many times these entities were called something else, like \textit{ultra light processes}, \textit{tasks} e.t.a. In fact they were all really just threads.

		Thread had been possible to program for many UNIX systems for quite some time now. But threads were considered a property a a process (and still are in many peoples mind). Especially among the UNIX community this perception is very strong. This has probably to do with that the only way to achieve threaded applications was to link in a thread library. In most peoples mind this library somehow magically give the programmer threading abilities. For all what it seemed to be worth, those threads shared the process environment, and threads \textit{"belonging"} to one process could not easily communicate with threads \textit{"belonging"} to another.

		There is no real or natural reason for this dependency between threads and processes. Unfortunately this has led to the missconception that this is the case. The only reason why this relation ever existed (because for all practical concerns it really did exist), was that the magic black box in the for of that library that was linked in was nothing else but a mini kernel in itself. I.e. each process would execute this own mini kernel that would in turn run all the threads.

		This is actually also possible with TinKer and some other embedded kernels (some call it collaborate mode - I prefer not to call it anything since there allready exists way to many words and names and concepts). It's just a kernel executing in another kernel, nothing really magical about that. It could be recursed in arbitrary depths if one would want to.

		In reality threads and processes are conceptually equal and dependency between them could either be there or not- That is totally dependant of the implementation. In Linux kernel since 2.5.7 with the addition of kernel threads (which is a missleading name b.t.w. since one would imply they would execute as UID root, which is not accurate) a process can create threads without using an external library. These threads are scheduled in the kernel as if any other process except that context switching between them is much faster.

		Another disadvantage with the older UNIX threading method was that time keeping was really bad. Suppose a thread were to go to sleep but during that time a whole new process would be switched into scope - this poor thread would have no way of regaining execution control no matter how important it would be. It would simply have to sit and wait until the kernel in the OS would let the process that in run the kernel of that thread to regain control. In reality this worked really badly and was almost useless. There would in fact be no particular advantages tread programming than program with processes, except maybe that the API was a little better and easy to handle.



		\subsection{Threads}
			Both processes and threads are execution entities. They are conceptionally very similar, almost identical.

			A thread is just as I explained before, a just piece of that \textit{"sawing thread"}\footnote{The analogy with program threads as cut-off pieces of a sawing thread was first mentioned in section \ref{genesis_thread} on page~\pageref{genesis_thread}.}, i.e. a small piece of the processors complete thread of execution. However, to be usefull it also has it's own context attached to it and the context to a thread is just it's own stack. On this stack, all local variables are stored there during is lifetime. Furthermore, when the thread is sleeping (or blocked as it's more accurately called), the stack will also store a copy of the contents of all registers that the thread had before it was put to sleep. This is so they can be restored when the thread is later awakened.

		\subsection{Processes}
			Processes were to my knowledge actually invented before threads (or at least sort of).
			I believe that during the time the first multitasking execution environment was invented by the guys at AT\&T, it was believed that a process should have all the properties of a CPU. During early days, tools were quite immature and software bugs were not only frequent, they were also very hard to find. Also, since hardware were immensely more expensive at that time, one \textit{HAD} to share the processor with other tasks and stopping the whole system if one process crashed was avoided at all costs. 

			The only way to solve this was to separate each process totally from each other. The context was expanded to contain \textit{all} memory types including the code segment. Now, unless the bug was part of the kernel itself, all other processes could (in theory) continue even if one certain process failed.

			In practice this didn't work out all to well in all cases. For a certain type of systems, for example national database servers or registry accounting systems this would work fairly well, but for embedded systems a failure in one part of the system would very often lead to a total system failure anyway.

			The only way to make truly fault tolerant (and reliable) embedded systems seemed to be to distribute them. This observation in combination with the fact that processes are much more costly to handle (i.e. context switch, start and stop) led to the usage of threads quite early on.\\

			\framebox{%			
			\fbox{Theads and processes are conceptually identical.}
			\fbox{All that differs is that the process context is wider.}
			}\par

	\section{stack}
		Suppose our kernel would be implemented as the simplified kernel in chapter 3. If two threads would be executing the same function, both threads will share that functions local variables. This is normally not what we want. We want that each thread executes in it's own environment with it's own copy of any local variables.

		For that we need one separate stack for each thread. A stack is just a piece of memory somewhere intended to store local variables and function call return addresses.
		\begin{figure}
		\begin{dotpic}
			node [
				shape=record,
				style=filled,
				fillcolor=yellow,
				fontname=Courier,
				nojustify="true",
				fontsize=10.0
			];

			edge [
				color="red",
				fontname=Courier,
				nojustify="true",
				fontsize=10.0
			];

			graph [
				rankdir = "TB",
				fontname=Courier,
				nojustify="true",
				fontsize=10.0
			];

			stack [ orientation=73.0, label="{\
				..  | \
				..  | \
				TOS --\> var 1         | \
				var 2   | \
				..  | \
				..  | \
				var n  | \
				ret address | \
				arg 1   | \
				..  | \
				..  | \
				BOS --\> arg n }"];
		\end{dotpic}
		\caption{A typical stack. Addresses from top to bottom.\label{stk1}}
		\end{figure}

		With our a kernel or OS, a CPU would only have one stack\footnote{Figure~\ref{stk1} shows the principle of a typical stack, with low addresses from top to bottom. In the figure TOS represent top of stack and BOS bottom of stack. When function calls are made, the arguments are first put on stack, then the return address. The function called upon is in turn reserving space for it's local variables on top of the same stack. A stack is said to grow upwards (imagine a hay-stack with more and more straws = data.) The order of the actual elements\footnote{Return address, function arguments, local variables} might vary depending on architecture and calling convention.} which is set during initialisation. When you have a kernel running, each thread (or process for that matter) will execute on its own stack. In principle all one has to do is change the stack pointer between all context switches.

		From a conceptual point of view, the stack concept is unimportant. You could in fact manage without all but the system stack (i.e. the one implicitly inbound by the fact that all CPU uses one), but it would be cumbersome to handle the mentined drawbacks. As a consequence, the stack concept is instead one of the most important entities of a kernel. We \textit{"only"} have to allocate some memory and to somehow attach that memory to each thread. 

		In theory this seems simple, but in reality this not trivial at all. This is also where most people fail, even if they've managed to come to realization as far as to this point. One of the reasons is that to do this operation is not "clean" i programmatical terms. This uncleanliness shows itself among others as need to implement parts in machine code language and towards architecture dependency. However, how dirty this becomes depends on the implementation. TinKer's implementation is to my belief as clean as such an operation can possibly be, and it involves in best case only two lines of architecture dependant code between various targets.

	\section{Time}
		The concept of time exists in many forms in a computer system: real-time, actual time, run-time, compile-time, clock, tick, RTC, process-time\ldots to name but a few. On-top of that, imagine that there exists different "schools" that often interpret one or more of these concepts slightly differently. The difference can be just a minor angel of view, but still have a very deep impact and consequence to the system that's based upon it.

		Time is both one of the best understood entities, and one of the most misused (or least understood) ones.

		To cover all aspects of time in a computer system, their meaning and implications would be daunting task, probably well deserved of a paper of it's own. We're just going to coves some aspect's and what those means to our particular case.

		One major difference worth mentioning is the difference between how we humans perceive time and how computers does is. We humans believe that we exist in a time continuum. Some of us probably also believe that time itself must have started at some point, and that it will stop at some other point. Also, we humans have no real problem to conceptually understand infinitely small differences of time and that two events must be differentiated in time no matter how close to each other they come. One must have been first, and the other must have been second (conceptually we know this even if it sometimes can be hard to see which one\ldots). We humans also often relate to time as absolute.

		All of the mentioned aspects above have a totally different meaning in a copmuter. Absolute time for a computer does not exist without the help of an outer entity telling it so. Just imagine, for a computer all time is relative to one event - then the computer got powered on.

		The other problem for a computer is to implement infinity. A normal computer can't do that since each value has to be stored in some sort of variable thats bound by at least it's representation. It has to trade either resolution or span. What consequence does this lead to? Even though we in any normal calculations get along very well with the value representations we have, time plays it's tricks on us. The reasons are actually quite obvious: 

		\paragraph{Drift} will be a consequence of limited resolution and accumulated error. No matter how small the error is, when infinity is a factor even the error will be infinite. Per definition it's therefore impossible to make any system that is without drift.

		\paragraph{Events} are actually a special aspect of time for a computer system. In many systems one can get away with not having any concept of actual time at all. The order of events could in that case be all we need to know. Some people even argue that time itself is just a continuous steam of separate time events, apart infinitely close to each other. (We'll use this analogy a little bit later in this text, but for now I'll recommend you not to dwell into this). However, one problem even for events also stems from resolution. Since a copmuter has to work with slices of time above infinitely small, there is also the ability (or disability) not to be able to detect which of two event's that occurred before the other.

		In other words: Time is not a very easy concept to make friends with for a computer system.


		\section{Dispatch \& schedule}
		In literature concerning operation systems and kernel technology one often encounters these two words, quite often interchangeably. However they are not the same and in TinKer we've chosen to be picky about these two concepts.

		\paragraph{Schedule:} The schedule in TinKer is a two dimensional array, with priority in one dimension and execution order in the other. Quite similar to a school schedule actually, with days Monday to Friday horizontally and time vertically. In the cross-point of a certain day at a certain time one would get information what one is expected to do. The similarity with TinKer's schedule is striking. In the crosspoint between a certain priority and the ordernumber of that priority one finds thread ID\footnote{A thread ID or TID is a handle to a task control block (TCB). 

		Each thread has one dedicated TCB and this  contains all information the kernel needs to handle the thread. The TID could be seen as an envelope, the TCB as the letter, and the content in that letter as the thread itself.}. Handling of the schedule is normally managed by the kernel itself and is called \textit{scheduling}. Scheduling is more or less planning the schedule, but not executing it. Many things affect the schedule, thread creation and destruction obviously, but also changing a process priority either temporary or permanently affects the schedule. A threads state does however not affect the schedule (i.e. wether a thread is blocked, ready or running), this is a matter of the TCB. 

		Some kernels have their schedules abstracted yet one level in queues, hence the word ready queue, which simply means that all threads determined to be viable for executing are put in a FIFO queue. TinKer does not do this for the following reasons: The FIFO order is allready inbound in the schedule itself since the second dimension is the order to be executed in each priority. All we have to do is letting the dispatcher remember which turn it had executed last time for each priority and then take the next one in turn. The second reason is that of speed and performance. If we were to have a queue, this would work fine if the schedule stays permanent. But what happens if the schedule changes and those changes happen to affect the threads in the ready queue? We would have to re-sort them every time and sorting is a relatively costly operation for a kernel not handling but a handful of threads.

		In big OS:es the situation is different. There you would normally have thousands of threads\footnote{processes actually, but we agreed that these were conceptually equal didn't we?}. Strange at it might sound, sorting is relatively less costly if managing many threads due to optimising algorithms. Also, the schedule itself will change relatively seldom, perhaps a few times per second. Whereas TinKer is made to be able to very effectively run master-worker models\footnote{I.e. implicitly create a lot of small theads with a very short lifetime - this is the operation that by far is the most demanding on the schedule} and for that programming model to be any usefull we have to have a kernel designed to be effective for that.

		As a comparison, I once made a master-worker model implementation sorting a text. It was a silly example, but each word got a dedicated thread whose sole purpose was to move the word it was responsible for into it's correct place. We were litterally starting and stopping several hundreds of threads per second. The same text was then sorted with the same program, one executing under Tinker's control, the otherone under Windows\footnote{with a special POSIX adaption layer, called \textit{pThreads Win32}.}. Guess which one was fastest? Sorting under TinKer took less than 1.5 seconds, whereas the Windows version took more than 27 seconds. Worth noticing is also that the CPU where TinKer was executing was a 40MHz XC167 nd the Windows one was a 3GHz P4 (!).

		In other words, some really beautiful programmatic concepts you can \textit{kiss bye-bye} on a system not well suited for it (which is a shame really).

		\paragraph{Dispatcher:} The dispatcher is a piece of code using the schedule. The schedule is it's "rule set" and the dispatcher dispatches the work based upon it. It does this by determining which thread is supposed to run next and also executing the actual context switch. One says that threads are "dispatched" according to the "schedule", i.e. they are executed one after one in an order determined by the contents of the schedule.
		
		In TinKer additionally keeps track of a order counter belonging to each priority. It does this so it can dispatch work evenly among threads within the same priority. This is needed to avoid the need of a ready queue.

		\section{events vs time}
		events vs time

		\section{Entry and exit points and yield}
		If we were just to start threads, the them execute until they're finished and the die by them selves life would be really easy, and writing a kernel would indeed be possible to do in 30 minutes.

		Even though we \textit{could} write our programs for that type of kernel, we normally wouldn't want to\footnote{At least one very obscure kernel I know of actually expects programs to be written this way.}. The reason is very simple but perhaps not so obvious. It turns out that it's just much more convenient to have threads living longer than just a fraction in time\footnote{Real-time temporal domain analysis using rate.monotonic is especially suited for this type of kernels} and it makes programming much more intuitive. The trade-off urged by the real-time gurus belonging to the temporal school is that you will not have control over which execution path each thread takes.

		This is however easily overcome by programming discipline. If you absolutely must handle temporal real-time requirements in software, you'd be really silly if the thread handling that requirement would be complicated. Besides, theres nothing stopping you using a thread with a simple run-though body, with only one start and one exit point\footnote{I.e. a simple action function without any logic (complexity) or synchronisation}\ldots

		Entry end exit poit's are places in the threads body\footnote{The function each thread operates on we call the threads \textit{body}. Note that several threads can share the same body. The body is only the rules for what to do and nothing else is shered except that. Each thread will have it's own copy of each bodies local variables. POSIX call this the start-function, but I thing body is better because just as a body can have several limbs, a thread can execute in several functions. Each of these functions data will be separated from other threads.} except the the obvious ones (entry and return), where the thread can get in and out of context\footnote{For now think of context as "execution environment" (or just "execution")}. 

		These points are in TinKer most of the function calls you'd use. This is also why the kernel works even on systems unable to preempt. A real time kernel kernel could manage without explicit entry and exit points, but then it would have to base all it's dispatching on preemption. Simulation\footnote{I.e. running you application on another target where the TinKer kernel would execute withing the scope of that targets OS} using such kernels would be very hard if not impossible. Debugging such program would be even worse\footnote{This is based on experience. Tinker is my third kernel, and both predecessors were preemptive-only kernels}. 

		In TinKer we actually have a choice to have preemption enabled only, never have it (i.e. dispatching only inside entry and exit points) or both. Each mode has it's strengths and draw backs, the combination of both potentionally enables us to have the best from both sides.

		\paragraph{Yield} Or yielding\ldots there is a special function intended \textit{only} as an entry/exit point and this one is called \textit{yield()}. It basically does nothing except calls the dispatcher\footnote{In thinker the dispatche is not a stand alone entity and the yield() function is actually part of the dispatcher}. 

		One normally uses the yield() function in parts of the code where you know execution has been going on for long without the kernel having had a chance to try to detect id some other thread is ready to run. Explicit yielding is quite uncommon\footnote{This is due to the fact that most applications will let it's system become idle most of the time. What better to spend that time than to let an dedicated idle thread run the dispatcher over and over again?} and the other API is normally quite enough. Exception to this would be in loops or other programmatical constructs, where the CPU would spend a lot of hard run time.

		

		
%------------------------------------------------------------------------------
%------------------------------------------------------------------------------
%------------------------------------------------------------------------------
\chapter{A thread comes to life}{
A thread comes to life in two stages. The creation of the thread and the staring of the thread. It's important to realize the difference between these two. This chapter will focus mainly on the creation. Start of the thread is covered in the chapter Dispatching in \ref{FIXME}.

\begin{itemize}
	\item Creation of the thread involves allocating and binding it's context. It also involves preparing each threads stack, so that it will be ready for dispatching.
	\item Staring of the thread is done asynchronously by the dispatcher. The dispatche will start the thread whenever it's run and whenever the schedule and priorities are proper for the thread to execute. The dispatcher doesn't know if what it's dispacing is just starting or has allready been executing a while.
\end{itemize}

As allready mentioned, a thread can be viewed just as a sort of a function\footnote{This function(s) we call the threads body. A bodies can be shared among threads.}. Or more precisely a function \textit{call}\footnote{because the thread is really an execution \textit{path} and not the function itself}. The only thing we have to do with it is attach a stack to it and we're ready to go. Or is there something else to it?
\\\\In principle, no - in practice, this is easier said than done!
\\\\The task of designing TinKer can be summarised into only 2 questions\footnote{The solution to each problem starts by knowing how to put the question.}:
\begin{itemize}
	\item Remember the little wile loop in our first program in table~\ref{hc1}\footnote{See page~\pageref{hc1} }? How can we make something that executes the same way, but doesn't start from within the inner loop?
	\item How can we attach a context to a tread\footnote{I.e. how can we connect a thread with it's stack}?
\end{itemize}
It turns out that the solution of both these questions depend on each other. I started by addressing them separately and fortunately I found a solution that satisfied both. I guess I was lucky\footnote{But not really...}\ldots


\section{Creating the thread}
\label{MakeOfThread}
	TinKer actually started as the program in table~\ref{hc1}. For quite some time I was fascinated by the beauty and simplicity by it. My previous kernels were much more complicated in this certain aspect, and I couldn't get rid of the idea that if I just could let that inner loop exist as a function somewhere else than in the main scope, I should be able to dispatch with exactly the same code. Guess what? It was possible!

	\paragraph{Resource allocation} 
		Creating a tread consists of two parts. Allocating the threads resources. This involves getting (or allocating) a TCB\footnote{Task Control Block - the threads \textit{"envelope"}}, allocating a stack and binding the thread, the TCB and it's stack togeather.

	\paragraph{Preparing it's stack} 
		Before we can run the thread we need to prepare it's stack. To realize why this is necessary, one must realize the very tight relation between this part of the threads life, and the dispatching\footnote{We'll cover dispatching in more detail in \ref{FIXME}} of threads. The stack, or actually the upper part of it, \textit{must look like as it would if the thread was swapped out after a context switch}. The reason being that it is the dispatcher that starts the thread and it doesn't do this any differently than any other normal context switching.

		In the preparation of the TOS\footnote{TOS (Top Of Stack) is used as a general term to refer to the whole upper part of a stack, not just the most topmost address. Note that each thread has it's own stack and that we hence have lot's of "TOS".}, three parts can be identified\footnote{It's not necessary to make this separation, but it makes a difference on the implementation since each of them implies certain special ways of solution}:
		\begin{itemize}
			\item Pushing the CPU current registers on TOS\footnote{Technically speaking this is not required, we only need to move the SP the corresponding distance to satisfy the reversed operation of the dispatcher}. We use the same routine that the dispatcher would use for half a context switch. This makes it easier to make certain that the stack looks the same the dispatcher would have left it after a context switch.
			\item Putting the return address to the start of the threads body at the correct place.  Put the threads starting arguments on the stack. \footnote{This step could be viewed as two parts (or steps), but they are so tightly coupled togeather that problems/solutions related to one also implies/affects the other. They belong together\ldots}	
			\item Copy the content of the CPU's current\footnote{I.e. after the above operations. Order is very relevant} SP to the corresponding thread's TCB "SP\footnote{A single-core system can only have one true SP, which is a property of the CPU}" variable. \footnote{This involves only 2-3 lines of code, but needs nevertheless to be written in assembly.}
		\end{itemize}
		These operations are made by a set of macros, why? Two reasons: $^{a)}$ The same reason as any normal program, you want to increase the abstraction level and thereby make it easier to read and logically grasp the code. Normally one would use functions for this\ldots $^{b)}$ Using functions would however be be awkward (if not impossible) since they them selves operate on the stack\footnote{Functions use the CALL operation, i.e. they PUSH, POP and BRET on the stack. Also note that tool-vendors do not lay out the "call stack" exactly alike.}.

		The choice is between hard coding this whole part of the kernel differently for each target, or to use macros as a fair compromise and just hard code those macros differently.

		During the evolution of TinKer the steps above were refined one by one and in innovative jumps. From having to implement all of the steps in hard code, to successively make each of them simpler and simpler. That todays latest solution in practice only needs to address the last point.\footnote{This is experimental as of this writing. But it's seems very promising and if it turns out OK - crude porting of TinKer to a new architecture would be really painless (days, \textit{not} weeks or months) and porting to virtually hundreds of CPU's would be manageable even by a small group of maintainers.}

		
	\subsection{Moving forward, by going backwards}
		To see the problem in the first question, consider the two programs\footnote{These are greatly simplified and would not work in reality. They just serve to exemplify the question.} \ref{hp1} and \ref{hp2} and ask youself what the difference between them is and how we can make \ref{hp2} work. One thing is obviously different. In the series of function calls, the last one is replaced with main\ldots

		If we were to look at the stack\footnote{so far we have only one stack, the system stack} in the middle of execution, and compared them with each other, we would find out that they are surprisingly alike\footnote{As a matter of fact they are conceptionally identical}. If we could just somehow get our program counter inside the loop in \ref{hp2}, the execution flow and the stack comparison would be "exactly" the same. However, our programming language doesn't seem to allow us to jump right inside. Actually, here is the first obstacle\footnote{You can't use C everywhere if you want to design a kernel.}, we need some sort of trick\footnote{But you should try very hard to use C as much as possible}. It seems obvious that we need to use assembly language to solve this problem. But should be just jump? And where to BTW, we don't seem to have a liable to jump to and surely we can't be expected to edit the output of the compiler just to add a liable each time we want to start a new thread?

	\begin{table}[!hbp]
	\begin{verbatim}
	#include "dofuncs.h"
	#define FOREVER 1
	int main( int argc, char **argv)
	{ 
	       while( FOREVER )
	       {
	                do_1();
	                do_2();
	                do_3();
	                do_4();
	        }
	}
	\end{verbatim}
	\caption{Hard-core schedule.\label{hp1}}
	\end{table}

	\begin{table}[!hbp]
	\begin{verbatim}
	#include "dofuncs.h"
	#define FOREVER 1
	int dispatcher()
	{ 
	       while( FOREVER )
	       {
	                do_1();
	                do_2();
	                do_3();
	                main();
	        }
	}

	int main( int argc, char **argv)
	{
	       while( FOREVER )
	       {
	           yield();
	       }
	}
	\end{verbatim}
	\caption{Crude dispatcher and yield.\label{hp2}}
	\end{table}

		No, we don't want to mess with that\ldots The answer lays in how the compiler executes a call\footnote{Hmm, lets see\ldots what was it that's different again?}. 

		Simplified a call to a function is done by a \textit{CALL}\footnote{some CPU architectures don't have any \textit{CALL} in their reportoar. The compiler will in that case construct a series of operations with the same effect based on for example \textit{JUMP}} instruction. This in turn leads to that the next address to be executed \textit{after the function is complete} is pushed on top on the stack. The CPU then jumps to the function and starts execution. 

		When the function is complete, the CPU executes a RET, which just fetches the address previously PUSHED by the CPU and enters that address into the program counter\footnote{The end result of completing this instruction cycle is that the CPU continues execution beginning at the address now in PC}-

		Howabout if we reserve this order in which a \textit{CALL} is executed? Let's try mimicking how a compiler would lay out the instructions . \textit{BUT} the other way around! Lets call the function by pushing it's address on top of our stack and then execute a RET instead. Now why on earth would we want to do that? 		

		Because $^{a)}$ we actually don't need to execute that RET\footnote{This will be done automatically when we exit the scope of the \textit{current} function} explicitly, and $^{b)}$ we can postpone the start of execution by letting the dispatcher \textit{start} the tread for us. The dispatchers job is just too seek up theads to run and then to change the context from the running one to the one to be run. 

		The beauty of it, is that \textit{the dispatcher doesn't know} that it's actually also \textit{staring} execution of threads now and then. All it does is enter a function called yield() and when that function \textbf{\textit{}RET}urns, the new thread will start executing. 

		This way a thread can be made to obey priority from the very beginning. I.e. if a thread is created with a priority less than the one of it's creator, it will \textit{not} start until the creator is ready to become blocked, thereby we don't create priority inversions\footnote{An unexpected bonus is that it virtually doesn't take any time at all to create threads. Time for creation and destruction doesn't have to affect the creator of the thread, who most oftenly is also a thread. Nice, we can now implement master-worker model based applications and don't need to rule this beautiful programming technique out because of poor performance!}. 

		In principle, all we need to do is pushing that address and the rest will take care of it self. Oh and yes, before we do that we also need to change the stack...

	\subsection{We need our locals}
		The second major problem was to attach a context for each thread. This is honestly not very hard at all, but we need another trick. The context for each thread is expected to be reached by \textit{one}\footnote{and one only \ldots} variable. This happens to be prepared by most CPU's allready and is called the CPU's SP\footnote{I.e. the CPU's \textit{stack-pointer}. This is a special register in the CPU that constantly points at TOS (Top Of Stack).}. So our context is pointed out by this SP?

		Not quite, but almost. Each thread has a TCB\footnote{Task Control Block, see \ref{FIXME} which is struct holding several variables. The most important of these variables are each threads SP\footnote{Actually, a copy of each threads SP as it were just before it got blocked. The real SP belongs to the CPU and there can only be one\ldots}} and this is the \textit{key} to the threads context.

		So what we do is\ldots 
		\begin{itemize}
			\item We allocate some memory\footnote{We use TinKer's internal \textit{'kalloc()'} for this, which is a managed \textit{'malloc()'} designed to be thread-safe and execution-time deductable. See later chapters regarding problems with stdlib and multithreading \ref{FIXME}}.
			\item We take the address of the last byte in that chunk of memory, and put this value into the threads SP \textit{copy} in it's TCB.
		\end{itemize}
		That's really all! Now when the dispatcher runs, it will find the thread to run\footnote{which in this case happens to be a newly created thread that wants to enter it's body}. it will look up that thread's context from the threads TCB and it will simply enter that value in the CPU's real SP. The top of that thread has allready been prepared\footnote{because we changed the stack to this one for a moment when we created the thread.} with space for all thee local variables reserved, but most importantly \textit{with the return address} and the threads start-up arguments. The thread with it's stack is now ready to enter it's body for the first time.

	\subsection{Why not facing front}
		Our thread staring technique\footnote{See \ref{MakeOfThread}} so far is elegant. It's very fast and it's quite\footnote{Well, sort of\ldots Very few but lines, but pure murder to implement} easy to implement. But is has a few annoying drawbacks, that all stems from the common denominator that the preparation of each stack before a thread can be run. It involves only a few lines\footnote{Typically between 50 and 250 lines of assembly}, but lines very hard to write. The reason was that besides having to learn each architectures assembly language., I also had to cope with different variants depending on the toolchain. And I had to learn each combination of tool-vendor and architecture there particular methid of calling convention\footnote{Once upon I time I thought there could be only one way. I soon learned that there are many\ldots (similar most of them, but still not quite equal)}. Added on top are the following differences concerning preparation of the top of the stack:
		\begin{itemize}
			\item They differ between architectures
			\item They differ between tools-chains (even for the same architecture)
			\item Calling convention might differ even for the same tool chain depending of how the tool-chain was compiled\footnote{I.e. which ABI it was configured with (ABI means Application Binary Interface. COFF (common object format file) is one example, ELF (extended linux format) another)}
		\end{itemize}
		For a long time I thought I could not possibly make the abstraction of a thread creation and dispatch any higher, and that this dependencies and obstacles would have to remain. I had to spend a few weeks to a 1-2 months for each architecture port, but it was still much better than my previous two attempts. 200 hundred lines of assembly code, how hard could it be\ldots

		The problem was not how hard it was. I could live with the fact that it took a while to create each crude port \textit{once}, but the maintenance issue was scary. I had by then TinKer already running on 6 different targets, and the time I ended up spending tuning and adjusting these lines soon became predominant. Also TinKer was very vulnerable for any small change the tool vendor came up with and I had to re-validate TinKer for each version of each tool-chain.

		There had to be a better way\ldots

		So I begun experimenting with the thread creation again. It turned out that I could let the compiler and CPU in combination set most of the stack up automatically. The solution was not as elegant as before, and I had to trade away a little time upon each thread creation\footnote{The automatic set-up is made by letting the thread execute for a short while. I.e. \textit{"compile time set-up"} was traded for \textit{"run-time set up"} }

		However, the number of difficult lines dropped to $1/5$ of what they where, and since I was spending most of my time in this jungle, I suddenly got much more time left for doing the fun parts.

		\paragraph{This is how the new technique works\ldots} 

		\paragraph{What about preemption?\ldots} 

	\subsection{Yet some improvements}
	\textit{This section descries an area not yet fully prooven.}\\\\
		During my contributing work for the GNU tool-chain, I had to implement some obscure functions called \textsl{setjump} and \textsl{longjump}. I've seen and read about them before, but they seemed awkward to use. Furthermore, they seemed to focus on error handling and other boring stuff. However, they were part of ANSI so we\footnote{i.e. the group helping out porting the GNU toolchain for \textit{Blackfin}} had to implement them. They were always also written in assembly because what they do is saving and restoring the contents of a CPU registers, which differs between architectures.

		Besides from a tiny little jump in both of the routines, they seemed similar to my \textit{PUSHALL} and \textit{POPALL} macros. Actually, they were conceptually equal\ldots and they are part of ANSI\footnote{I.e. No matter version, tool or vendor, you should \textit{always} have a working set, adapted for the CPU architecture and any imaginable variants of that architecture}\ldots
		\\\\
		HEY!\footnote{If it proofs to work out all right, a crude ports will only differ by two lines of assembly between each other. Suddenly all the ports the GNU tool-chain supports is also potentially availabe for TinKer!}

}%Chapter

%------------------------------------------------------------------------------
%------------------------------------------------------------------------------
%------------------------------------------------------------------------------
\chapter{The kernel scope}
The kernel scope is the adapted environment that makes dispatching, and scheduling possible. It is the outer bounds in whichthe CPU executes.
\\\\
In other words, it's the whole \textit{"program"} as seen by the CPU. 

Remember I said that there is "no difference between a normal program and a threaded program, it only appears that way"? This is part of what the kernel scope is.

The kernel scope would be best understood if viewed from two perspectives: CPU and kernel.

\section{CPU perspective}
	From the CPU perspective the kernel scope encapsulates the tinker kernel and all it's threads. It's \textit{the program itself} and could be compared with a \textit{process}. Except that most target's that TinKer is supposed to run on doesn't have any 

	If anything should be called a process it would be the CPU bare-boned execution environment itself, i.e. the \textit{processor}. 
	\textit{process}.%
	\marginpar{%
	\it{processor = process}
	}%
	However, when you run any TinKer based application under for example Linux, the application will indeed execute in a process of it's own - i.e. a Linux process in that case 

\section{Kernel perspective}
	From the kernel perspective, imagine what the kernel would need to be able to run at all. We say that to be able to execute, we need to instantiate the kernel. This is consists of the following tasks.

	\subsection{Allocation}
		One of the main design philosophies behind TinKer is that the kernel shold not rely, trust or depend on any specific libc\footnote{"C" standard library} implementation. However, we don't want to reimplement everything either and would like to take advantage of whatever support the standard library can give us.
		\\\\
		What's the problem then? The problem is that standard libraries doesn't normally consider any concurrency or multitasking. Furthermore if any particular library do, it's always tightly coupled with a certain kernel. We don't want to reimplement any stdlib, basically because in the case of the client using commercial tools we can't (or we would have to rewrite the whole thing), and in the open source case because we would have to administrate, manage and maintain patches witch is a lot of work. So what we do is \textit{coping} with the standard library you're using.

		The only thing that's in the way of using \textit{any} working standard library, is the matter of concurrency. I.e. if we use the library in a non-concurrent way we're OK. This is what TinKer does. \textit{TinKer \underline{preallocates} any resources it needs, and all the resources the application is expected to need\footnote{This is the reason why TinKer needs all those configuration parameters when you build it.} before execution of the application is started. These resources are then handled internally by TinKer's own API, which takes care of any concurrency issues.}

	\subsection{Set up}
		When all the resources are allocated, we need to set the kernel up. This involves clearing the schedule, initialising all TCB's in the pool and to create the idle thread.\marginpar{\it{main resource = stack = memory}}

		Now the current thread of execution (i.e. the CPU total thread of execution) is \textit{"re-assigned"} a context making it appear like any other thread that can be part of our schedule\footnote{Now two threads are in the schedule, idle and root}. This "thread" will be named specially and called "root" and will be given the system highest priority\footnote{Thereby guaranteeing execution until the user program starts it's own first thread}. It's resources will also be based on what was available in the kernel scope, and not allocated as for the rest of the threads\footnote{I.e. it inherits the stack from the kernel scope}.

		Last of all the root's body is entered. This function is expected to be named \textit{'int root()'} by the linker, but is converted by a macro trick to look like any normal program entry point, i.e. \textit{'int main(int argc, char **argv)'}. 

		I.e. you're program can use either of the two, but it must have one. I would recommend to use the second notation because it makes your program directly portable to any other POSIX environment.




%------------------------------------------------------------------------------
%------------------------------------------------------------------------------
%------------------------------------------------------------------------------
\chapter{Dispaching}

\begin{figure}[!hbp]
\begin{dotpic}
	node [
		shape=record,
		style=filled,
		fillcolor=yellow,
		fontname=Helvetica,
		nojustify="true",
		fontsize=10.0
	];

	graph [
		rankdir = "LR",
		fontname=Helvetica,
		nojustify="true",
		fontsize=10.0
	];

	edge [
		color=black,
		fontname=Helvetica,
		nojustify="true",
		fontsize=10.0
	];

	Dispatcher -> IO		[label="You're next"];
	Dispatcher -> Console		[label="You're next"];
	Dispatcher -> Application 	[label="You're next"];

	IO -> Dispatcher		[label="Yield me"];
	Console -> Dispatcher		[label="Yield me"];
	Application -> Dispatcher	[label="Yield me"];
\end{dotpic}
\caption{Dispatching inprogress\ldots\label{disp1}}	
\end{figure}

\section{Where, what, who?}
Dispatching is the activity where reading \& interpreting the schedule and context switching takes place.

So far we're talked about \textit{"dispatching"} and \textit{"the dispatcher"} as if there were a certain entity in the kernel for that. It doesn't\footnote{At least not one dedicated entity}\ldots

Certainly a kernel \textit{could} have a dedicated dispatcher, and I'm certain that some kernels do. It outlines a very nice abstraction and it could be implemented more or less exactly as our very first imaginary kernel in in table~\ref{hc2}.

However, though this seems nice in principle, there are a some penalties to be payed if implemented that way.
\begin{itemize}
	\item $^{A)}$ The dispatcher would have to have it's own context
	\item $^{B)}$ It would have to run at well defined occasions
	\item $^{C)}$ It would have to run at highest priority
\end{itemize}
Alltogeather, this creates a rather clumsy runtime behaviour. Unnecessary time would be spend running the dispatcher when it's very probably not needed. It would also \textit{require} preemptive run-time behaviour, thereby making it much more difficult to run the kernel in a simulated environment.

When the process\footnote{meant in it's literal sentence, i.e. not as a operating system process} of dispatchting takes place can be divided in the following three philosophies\footnote{This is one of the most important foundations for any  kernel and will have one of the greatest impacts on the design}.
\begin{itemize}
	\item On a regular basis, often driven by the system \textit{"tick"}footnote{Usually as part of the timer interrupt which otherwise only increases a counter variable called \textit{systimer}}. I.e. time is regarded as an series of events with a certain granuality{A term used in computer science meaning how "fine grained" time can be perceived as}, and the dispatcher is driven by those events.
	\item Similar to the above, but more generalised meaning that every outside stimuli (i.e. interrupts) will run the dispatcher. This is the case in traditional real time kernels. Note that time could, but doesn't have to, be regarded this way. When you have the abilities to run the dispatcher on other stimuli, you don't need the time to help you cut the CPU's total thread of execution in pieces any more. Wheather it's done or not is implementation specific for your kernel in question.
	\item The application (i.e. the threads themselves) besides when and where. This is done by explicit calls to a certain $yield()$ function, or by entering certain entry/exit functions\footnote{Usually all of the kernels public API is a potentila entry/exit point.}  This is called \textit{explicit yielding} dispatching.
\end{itemize}

Which of these philosophies to use depends on what type of kernel you want. All of them can potentially satisfy the first part of the real-time paradigm. But not all of them can satisfy requirements in temporal space. TinKer uses all of these philosophies, but with an emphasise on the last one. The main difference between the first two and the last is that the formers leads to preemptive behaviour and the latter does not. 

Many kernels follow one of the above philosophies quite strictly. The reason being that each method affect the design very much deep down in the kernel. Their respective solutions don't have much in common either\ldots. However, one doesn't \textit{have} to rule one on behalf of another\textit{theoretically speaking}. Question is rather if it's practically possible \& justifiable or not. 

\textit{Explicit yielding} is the fundamental type that formed the design of TinKer. It turns out that going from this type of kernel dispatching towards any of the other two is much easier than the other way around. One will end up with a quite lot more code than if any other would had been addressed specifically and exclusively from the start, but the code is quite readable\footnote{Going the opposite direction would be a complete mess to say the least.} and trading between readability and performance always goes in favour to readability\footnote{Many will not agree, but use common sense and ask youself: How many obscure OS:es or kernels out there do you know that's survived more than 2 decades?}. Focusing on readability leads to maintainability, and if some speed or effectiveness (if any) is lost that way, so be it. Computer hardware has a tendency\footnote{This interesting \textit{"fact"} has been true for more or less as long as computers has existed} of doubling it's performance by 100\%  every 18'th month but a kernels dispatching method will stay static for it's entire life span.

The design of TinKer is influenced by the  empirical observation, that of that most of the time a computer system runs, it does nothing but sits and waits\footnote{I.e. executes in the \textit{idle loop}}. I figured, as well ass sitting there in a empty while loop doing nothing, it's might just as well do something, even if that most of the time turns out to be equally unnecessary. Why not \textit{"dispatch"} there instead over and over.\footnote{In the idle loop? You must be crazy - this can't work!\ldots}

If this were the only place dispatching would take place, this would obviously not work very well. Fortunately this is not the case either. Dispatching takes place on many places\footnote{Depending on how the kernel is configured: Preemptive only, non-preemptive or combined}, most notably is at the \textit{entry of each exit-point}. I.e. every time you code is making a system call that belongs to TinKer, the dispatcher is run. This is what makes concurrency in non-preemptive environments.

All these calls have one common dominator, and that's the yield function\footnote{more or less embedded deeply in each function}. Yield is where the actual dispatching takes place but you still can't call it the dispatcher because it belongs to no-one and anyone can call it. I.e. dispatching is a property of the whole combined system \footnote{I.e. the kernel and it's threads}. If some entity would be pointed out as a dispatcher, it would have be the whole kernel. Notice the two main properties of the dispatcher
\begin{itemize}
	\item Determining who's next to run and run it\footnote{i.e. context switch to the thread in question}.
	\item Repeat this in a never ending loop
\end{itemize}
The only practical difference between TinKer and the small little example in table~\ref{hc2} is that this loop is extended. It's still never ending though\ldots

\section{The sleepwalker}
This method of achieving concurrency seems really silly doesn't it? TinKer claims to have real-time abilities and very fast response times, so how can this be even remotely possible?

First of all, running yield function is not as costly as one would think. Naturally doing it to often would put an unhealthy ratio between CPU time spent in the application and in the kernel, but you won't in most cases.

To realize this you have to realize the difference between the execution path and the program code. The distance between two entry/exit points might seem close in your program code, but in reality they are relatively speaking usually very far apart.

The second thing that motivates this type of kernel is based on the same sleep observation as mentioned before. It is very likely that whenever something happens\footnote{I.e. when an event happens per definition} that the CPU will be executing in the idle thread happily executing yields all the time. I.e. the probable response time will be lightning fast since we don't have to wait for the dispatcher to specifically be run.

\textit{\textit{"Probability"} based execution! But what about that \textit{"real-time"}?} Here's where the kicker line comes in: Speed and response time hasn't anything to do with real-time in a system not having temporal requirements. Only event's matter and the order between them. This is quite likely the only think you need\ldots

There are a couple of really big advantages using non-preemptive dispatching.
\begin{itemize}
	\item It's much easier to debug (compare preemptive debugging as trying to debug interrupt functions). It's very hard and human error and further conclusive misstakes are eminent.
	\item A preemptive kernel never 100\% safe. Reason is that it's very hard to make a good compromise between where interrupts might occur, and where they must not. Playing it safe but by disabling too much would lead to poor behaviour, opening up could mean you miss a spot that's critical. This are is furthermore complicated by the fact that while inside the kernel, the kernel can't use it's normal protecting mechanisms. It's has to rely on HW disabling of interrupts and this is awkward\footnote{The awkwardness is due to that if you want to open up preemption in some sort of complex part of the kernel, you almost always need to know the state of the interrupt disable flag. However, determining this will take yet a few cycles during which time another interrupt might have occurred. How do you know which? Some HW have support for atomic operations of read-modify-write settings of flags however.}.
	\item From practical experience, I know that being able to run a control application in a simulated environment is a big advantage. Depending of the application the possibility to do this varies of course, but it's usually much more common that you'd think\footnote{Consider if you application has parts that could be run in simulated environment even if other parts can not}. The ability to run in a simulated environment is usually very rewarding and your investment in terms of adaption and set-up time is normally payed back with enormous profit.
\end{itemize}

\section{Have it your way then}
However, should you have temporal requirements - TinKer can satisfy these too. You would have to start using interrupts however. If you feel confident with that, then there is nothing stopping you.

When you start use TinKer in a preemptive way, you will be able to address temporal space. But also please note that the response times will resemble those of a normal kernel much more. No more light speed response times with optimistic success rate. Instead you'll get a little slower but deterministic temporal behaviour.

When TinKer is compiled without any options regarding preemptiveness, preemptiveness is actually enabled. Whether you use it or not depends if you use ISR's that call TinKer kernel API.

However, if you plan to use TinKer for temporal requirement I would suggest you compile it with with preemptiveness exclusive\footnote{Build option --enable-dispatch=EXCLUSIVE}. The reason is that you don't benefit from \textit{using} both preemptiveness and non-preemptiveness behaviour at the same time.

What will happen when the kernel is built that way, is that several of the kernel API that were previously also entry/exit functions will abandon that duty and do \textit{only} what their definition says. This leads to that execution time for each thread from event to response is much easier to deduct (and that's what you need to formally validate your temporal requirements).

The time keeping function will also change to the \textit{PTIMER} module instead of the \textit{sleepy timer}\footnote{\textit{PTIMER} is a preemptive timing technique, whereas \textit{"sleepy timer"} is poll based}. Last but not least, it will lead to that the idle loop will not execute yields anymore, thereby making sure that the time spent in the kernel by the CPU also become minimal and deductable. 

I.e. the CPU will execute inside the kernel only when you tell it to, and time for each execution path can be determined and entered in you rate monotonic analysis.
\\\\
I would urge you to carefully consider if this is for you. True temporal domain problems are something for experts.

Yet another aspect is the following, which is the same reason why true temporal requirement applications are quite uncommon in the UNIX world\footnote{Relatively speaking but even among control applications}. Temporal space is a matter of the whole system. The more complicated the system is, the harder it is to verify. Do you have control of all parts? Are all threads in you're control? Do you even know about them? Can you trust the execution environment?

I.e. even if your timely analysis is OK and you can formally prove your temporal requirements, it's still possible you've missed something that will crash your timeliness, and furthemore: you'll not know it unless it's happening\footnote{I.e. too late}.

So unless you're not 100\% certain you have full control of each part of your system, I would urge you to rethink.

I'm not joking, but the 30 minute kernel might not be such a bad alternative afterall\footnote{See \ref{kernel30}}.

\section{Preemption, events and other stuff}
\marginpar{$Preemption \ne ISR$} Sometimes people confuse preemption with event driven programming, by thinking events are the same as interrupts and interrupts always mean the same as preemption. This is not the case. In fact as table~\ref{pree_ev} will show you, you can use event driven programming technique in a non preemptive kernel, and you can have pollong driven programming while still benefitting from preemption. \\\\
In TinKer most programming techniques work either you use preemption or not and that you don't need to have preemption just to detect event's. Polling used the right way is in many situations both better safer and more predictable. You can also still make use of interrupts even in non-preemptive mode. In that case the thread affected by the even in question will not start executing until the ISR is exited and somewhere else in the normal program an entry/exit point is reached.

The difference is not in the programming techniques, and programs could in fact look exactly the same for the two cases. The difference is in the temporal behaviour of the system. 

Consider the two programs in table~\ref{isr} and table~\ref{poll}. They each represent event  two different ways of interacting with the outside world.

\begin{table}[!hbp]
\begin{verbatim}
unsigned long cbuff[4];
void ASC0_viRx(void) interrupt ASC0_RINT
{
   cbuff[0] = ASC0_uwGetData();
   q_send(tk_sys_queues[Q_SERIAL_0_I],cbuff);
}

unsigned int thread2(void *inpar){
   unsigned long msg_buf[4];
   int depth = 0;

   printf("Thread 2 started with prio 5\n");
   while (1){
      q_receive(tk_sys_queues[Q_SERIAL_0_I],WAIT,0,msg_buf);
      depth++;
      printf("Thread2 (%d) received fron stdin: %c\n",depth,msg_buf[0]);
   }
}

unsigned int cpu_hog(void *inpar){
   int depth = 0;
   int k,l,x,y,z;

   printf("CPU \"hogg\ started with prio 7\n");
   while (1){
      for (k=0;k<10;k++)
         for (x=0;x<1000;x++)
            for (y=0;y<1000;y++)
               for (z=0;z<1000;z++)
                  l = (((x+y+1)*k)/z) % 10;
    depth++;
    printf("Hogg completed round: (%d): %d \n",depth,l);
   }
}

void root(void){
   clock_t latency = 0;

   printf("Hello world o f  TinKer targets\n");
   tk_create_thread("T2",    5,thread2,1,0x600);
   tk_create_thread("HOGG",7,cpu_hog,1,0x600);

   printf("Root started\n");
   while (TRUE) {
      latency = tk_msleep(10000);
      printf("Root \"bling\"!\n");
   }

   tk_exit(0);
}
\end{verbatim}
\caption{Events determined by interrupts.\label{isr}}
\end{table}

\begin{table}[!hbp]
\begin{verbatim}
#define PERIODT 100
unsigned long cbuff[4];
void ASC0_pollthread(void)
{
  clock_t latency;
  
   while (TRUE) {
      latency = usleep(PERIODT-latency); 
      if ( ASC0_uwHasNewData() ) {
         cbuff[0] = ASC0_uwGetData();
         q_send(tk_sys_queues[Q_SERIAL_0_I],cbuff);
      }
   }
}

unsigned int thread2(void *inpar){
   unsigned long msg_buf[4];
   int depth = 0;

   printf("Thread 2 started with prio 5\n");
   while (1){
      q_receive(tk_sys_queues[Q_SERIAL_0_I],WAIT,0,msg_buf);
      depth++;
      printf("Thread2 (%d) received fron stdin: %c\n",depth,msg_buf[0]);
   }
}

unsigned int cpu_hog(void *inpar){
   int depth = 0;
   int k,l,x,y,z;

   printf("CPU \"hogg\ started with prio 7\n");
   while (1){
      for (k=0;k<10;k++)
         for (x=0;x<1000;x++)
            for (y=0;y<1000;y++)
               for (z=0;z<1000;z++)
                  l = (((x+y+1)*k)/z) % 10;
    depth++;
    printf("Hogg completed round: (%d): %d \n",depth,l);
   }
}

void root(void){
   clock_t latency = 0;

   printf("Hello world o f  TinKer targets\n");
   tk_create_thread("T2",    5,thread2,1,0x600);
   tk_create_thread("HOGG",7,cpu_hog,1,0x600);

   printf("Root started\n");
   while (TRUE) {
      latency = tk_msleep(10000);
      printf("Root \"bling\"!\n");
   }

   tk_exit(0);
}
\end{verbatim}
\caption{Events determined by polling.\label{poll}}
\end{table}

