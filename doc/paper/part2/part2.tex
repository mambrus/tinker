\part{Design philosophies \& considerations}
\chapter{Introduction}
This part will elaborate the design philosophies that is the foundation of TinKer.

One could see these a requirements specification, but I'd rather not. Requirement's in a development project is not scientific per definition, because to be able to set a certain requirement up you really must have a certain idea if it's doable or not, how costly it would be to implement and at least a very general idea of how to do it.

These point's are one step ahead of this. Instead of putting up some inflexible requirements, I put up certain ambitions that I believe are of some certain value to the project. Most of them are not easily verifiable, but still certainly have a deep impact to the end result.
\\\\
It's a matter of prioritising and to figure out what you think is really important\ldots

\chapter{Readable \& Maintainable}
The idea was from the beginning to write a kernel in togeather with a thesis. I allready knew it could write a kernel and also in principle how. Readable and maintainable are closely related terms. 

The idea to back it with a text was tho aid me in seeing the difficulties from the readers perspective. Kernel design is a difficult area to teach. The problem is almost always to "communicate" the ideas and not the techniques themselves\footnote{Because kernels do some "dirty" tricks that cant be described in normal programmatical terms}. 

Once you get over a certain threshold, you'll consider this easy as pie. Problem is to get over that threshold\ldots
\\\\
When one decides upon a requirement like \textit{"readability"} one really doesn't know what one's committing for. How can you make something readable that can't be red from top to bottom? And how do you \textit{measure} this readability?
\\\\
Readability is making everything as clear as possible and not to introduce complicating factors if they can be avoided. Sometimes this aspects goes beyond just choosing between two seemingly equal ways and in some cases a choice has to be made between readability and some other technical aspect\footnote{Performance is something often in clash with readability}. In all cases I've chosen readability because of the main reason of doing this work at all, and for the convince that hardawe will catch up much faster than kernels evolve or new kernels come out 

Readability also has several other very nice secondary effect\footnote{But still very very important} in that TinKer is very easy to port, and hopefully many people will help in doing so.

Porting is an aspect of maintainability. In general you can say that the cleared a code gets, the more maintainable does it also become\footnote{up to a certain level}. However, maintainability is also dictated by the broader technology choices you make. In TinKer's case the idea of having a software based schedule is one example. Common to these decisions are that they are so deeply embedded in your design that they are practically irreversible. They do however define some sort of limit about how maintainable a project can become.

Sometimes maintainability can't only be measured in time spent to do for example a certain bug fixes or any other kind of maintenance.

For example, we've decided that TinKer should also be tool-transparent. This requirement drains a lot of time and work because of the time it takes to verify and test all targets based on these different tools. It's painfull process and it would be much easier to support only for example the GNU tool-chain\ldots

On the other hand, there is a certain value in that TinKer can do this at all. Most kernels can't and the indirect result of that is the gap between embedded developers for small targets\footnote{Devices, 1 CPU based} and traditional embedded developers\footnote{Big system, but still embedded. Traffic control e.t.a.}.

To make a project like this readable and maintainable is not an easy task. You cant address this issue like a normal requirement, like when your requirement is broken you just file a bugrapport and "fix it". The issue is seldom isolated, and I had to re-write certain parts of TinKer many times, not because it wasn't working but because I wasn't happy with the readability.

Basically this is how it's been done, a slow and evolving\footnote{I.e. you don't know exactly how do do it. Instead you permute different solutions until you find one that's satisfying} process, involving occasional re-writes and even re-designs.

\chapter{Standardised API}
One of the most important ambitions in TinKer was to base it's API on well established standards.

The reasons for this are many, but I'll mention a few
\begin{itemize}
	\item Much easier to start using
	\item Thought trough by experts 
	\item Literature is full of books, which saves saves me the a lot of work
	\item Non standard kernels are evil	
\end{itemize}
Tinker follows POSIX standards 1003.1c\footnote{pThreads} and 1003.1b{Only part of. I.e. RT queues and semaphores.}. The ambition is to follow \textit{parts}\footnote{Since TinKer is addressing the small embedded system, we can't cover too much. The important thing is to cover the right parts.} of these standards and not to add any propriority API.

This is important, because it means that $^{a)}$ TinKer will be possible to implement for small to very small targets and still have the important API, and $^{a)}$ you as a developper can transfer your projects as to a bigger and more powerful target as you're needs grow.

The freedom of being able for the user to chose a kernel for the right reasons are very near and dear to me. I want users to keep using TinKer because it's good, not because it's a irreversible choice.

\section{POSIX 1003.1c}
POSIX 1003.1c or pThreads is a fairly recent addition to the full\footnote{POSIX is \textit{HUGE}. It covers all from file-systems to shell and terminal behaviour} set of POSIX standards. describe threads in a UN*X operating system. This is a bit awkward, because tinker is not UN*X and certainly not an full operating system.

However, it's a very good standard, and besides of the above unfortunate and a little bit missleading terminologies, it can be very well adapted and suited for a small embedded system.

TinKer implements almost all of POSIX 1003.1c and the ambition is to cover it fully.

When it comes to similarities \& differences between this standard and TinKer one has to bare a few things in mind:
\begin{itemize}
	\item TinKer is \textit{not} an OS, it's a \textit{kernel}. Many vendors targetting the embedded market call their products \textit{"OS"} which is really not accurate.
	\item There exists no processes in TinKer and the minor parts concerning threads between different processes communicating is not applicable for TinKer.
	\item POSIX 1003.1c is a standard describing an API for concurrency, it does not mention real-time. TinKer is a real-time kernel based on this API. I.e. depending on the real-time requirements, you might be limited in your choices of alternative kernels/OS:es from a portability aspect\footnote{The alternative for the end user would be to use a proprietary API which would be even more limiting}. I.e. you have to find another \textit{real-time} kernel following the same standard\footnote{I.e. there exists kernels/systems that follow POSIX 1003.1c, but that are not real-time\ldots} 
\end{itemize}
\section{POSIX 1003.1b}
TinKer implements parts of POSIX 1003.1b, namely queues and semaphores\footnote{\textit{mqueue} and \textit{sem}}.

This standard is older than POSIX 1003.1c, which is quite funny actually. In my point of view it's a good example of the gap between the \textit{"deeply embedded school"} and the \textit{"old embedded school"}, since this standard among other thing mentions real-time, yet not realizing what real-time abilities depend upon.

So, since it's an older standard one would have to assume that there must have existed some implementation for it somewhere. It would be interesting to see how "real-timeish" this implementation would be.

Some traces can also be found in the API itself\footnote{This is not the case for the API in POSIX 1003.1c, which is a much more modern API}, since it suggests there should be a filesystem somewhere, which is just another unfortunate missconception from the UN*X community. There certainly does not have to exist any filesystem to handle for example queues. However, the standard demands it, so TinKer simulates it's existence.
\\\\
There was no reason to follow the same order of events as the historical order. TinKer implemented pThreads first, and then based the POSIX 1003.1b queues and semaphores on top of that.

The reasons were as follows:
\begin{itemize}
	\item Queues and semaphores are just aspects of the same thing, synchronisation
	\item A system needs only one synchronisation primitive, on which all other can be based on.
	\item pThreads has such a primitive, very awkward to use though.
	\item Some systems I want to use as reference, support pThreads but not POSIX 1003.1b. For these I can use this package as an isolated part.\footnote{For example, if you want to try the examples on a Cygwin Win32 target, you would need TinKer's queues and semaphores. Cygwin is a fine piece of code and I use it myself very often, but it still lacks some parts\ldots}
\end{itemize}


\chapter{Small systems without compromise}
As I mentioned there exists a gap between small systems embedded designers and old school embedded designers.

These two sides have been apart for decades and not being able to come closer each other very much probably because they don't speak the same language\footnote{in technical terms} or because of their backgrounds and traditions are different. Developers targeting small embedded control systems very often have a electronics background thats \textit{"evolved"} into computer science, and they usually have certain difficulties seeing the \textit{"major"} benefits of programmatic methodology, various kernels, standards and what-not.

On the other hand, these guys have a common sense when it comes to what a endproduct is and what costumers believe it's worth paying for.

My ambition is to help these two worlds come closer to each other, by providing a good kernel to the small embedded community\footnote{note: Only the targets are small - the community itself is huge!}, but one that enables programmatic terms that is accepted in the more advances computer science community.

Like in for most of the points in this section, the solution to this issue has the sample properties in terms of evolution, and re-write. However, one aspect make it a little bit to handle, and that is to make ones mind up about what \textit{not} to address:
\begin{itemize}
	\item No filesystems or file based device driver concept
	\item No processes $\Rightarrow$ i.e. no memory protection/handling
	\item End-target support only in terms of over-all architectural ports
	\item Not re-inventing standard library more than necessary
\end{itemize}
I.e. I have a certain type of system in mind to begin with, then I'll implement what's needed for that system and not more.
\\\\
To give you an idea of what that system could be like:
\begin{itemize}
	\item A CPU/MPU with 64k\footnote{CISC CPU} to 128k\footnote{RISC CPU} ROM/Flash
	\item RAM consumption \textit{staring} at 8k-16k
	\item An automotive subsystem like an engine control
	\item High processing power
	\item High volume product with few parts
\end{itemize}
As a rule of thumb, RAM is more important than ROM/Flash. A kernel is a major memory hog when it comes to RAM. In Tinker's case, RAM limits the number of either threads, queues or other kernel resources. TinKer does all resource allocation during startup, but detection is  still in run time. Besides, it will not cover the case if you need to further allocate resources in your application, where you could still get a resource starvation problem that you have to validate on a running system. I.e. if you have an chance to design using more RAM, please do. RAM is cheap and having to much is just not possible by definition ;)
\\\\
ROM limitations are more distinct and deductable in this sentence. If your binary fits when you load it, it will run.


\chapter{CPU architecture transparency}
\textit{The holy grail \footnote{How does one make a kernel CPU architecture transparent? Answer: you can't. But you came more or less close\ldots}\ldots}\
\\\\
One original idea was to see how CPU transparent we could make TinKer. This is not an extremely important issue, because all it means is how long time it will take to make a port. It says nothing about either the kernels technical characteristics nor the ports.

However, it plays a indirect role. If we could make the kernel CPU transparent, there would also be a much greater chance that TinKer got widespread. Thereby creating a bigger community, who in turn would aid in indirect testing and debugging.

To decide to make a kernel transparent as a requirement is just ridicules. One simply doesn't know how much this \textit{"beast"} can be abstracted before one tries. However, havingthis in mind from the start, letting you influence every decisionmaking you do, will lead to something that's much more transparent that not considering it at all.
\\\\
Again, this was a slow maturing process, but the end result is pretty promising\ldots 
\\\\
TinKer has all architecture specific code in in-line assembly macros. These assembly lines are extremely hard to write, since these cover not only architectural difference, but also tool-chain differences\footnote{Calling convention, in-line assembly syntax}. However, number of lines has dropped from an average of 300 lines to just a very few\footnote{Latest experimental code contains only 2 lines of architecture specific code!}. Having only a few architecture dependant lines of code opens up the ability to provide many more ports.

In reality TinKer will easily let itself be ported to all embedded targets that the GNU-newlib library has been ported for, which are quite a few.

\chapter{Tool-chain transparency}
First of, what does this mean?
\\\\
This is a terminology that I'll use in this text to cover a certain aspects that a tool-chain might inflict on a kernel. Most of the kernel is written in C language, and one would think that since C is standardised in itself, this would be enough, This is not the case however\ldots

\paragraph{Context switching} involves the operaton where one has to understand exactly how a function call is performed, so that when the CPU returns from a certain contex switching function, it will in fact return using another threads saved context\footnote{And part of that context is also the "new" return address}.

Different compilers execute a call differently and there are no strict overall rules for this. There exists standards\footnote{Called ABI = Application Binary Interface}, but very commonly these are honoured by only one or a very few vendors.

If this vendor is a dominating party, thus would be OK and we would only have to do one special adaption. But since TinKer addresses an area where there still are no dominating vendor, he have to cope with the fact that there might exist many.

Context switching can be done in many ways, but it's actually not the context switch itself thats the issue. The problem is in creating the thread, since we then need to mimic how a function-call will look on the stack, so that when the dispatcher when it is finally doing context switching will have the newly horned threads stack in a way that is compatible to the dispatches normal\footnote{i.e. context switching with threads previously switched out of context by the dispatcher itself} operation.

This particular area has been one of the toughest come up with a good solution for, but I think we now have a fairly good solution.

\paragraph{Header files \& structure} are usually considered part of a standard library. In fact, the which standard library you use could be viewed as part of the tool-chain\footnote{In this context, the tool-chain means C compiler, assembler and linker}. In this text I've chosen to separate them apart because a compiler can theoretically work with different standard libraries\footnote{For the GNU tool-chain use either Glibc or Newlib, depending on if it's a dedicated embedded target or not (newlib)}.

How various tool-chain separate themselves from their respective standard library differs greatly. In some case a clear separation can't be determined\footnote{Keil} and the two are heavily intertwined, in some cases there exist a fair separation\footnote{GNU} and in some cases the separation is total\footnote{CADUL among others}.

Usually a compiler can't get \textit{completely} free from a C library and some small fragments of code are still needed. For example, GNU needs at least a few modules to help it start up the code. It also uses a few macros to handle endianness and floating point operations are. Furthermore, the GNU tool-chain will copy all the header files from it's corresponding library to a certain place meant to be accessible by the tools.

Very often these default headerfiles clash with TinKer's, and we need to use our own. This is handled by a set of nifty macros. However, to work it requires from the user that the search-patch for system headerfiles come AFTER the search path for TinKer's corresponding files. 

This applies for both application and for building the kernel itself. Failing to do this will in best case be detected in compile time. So far I've never seen a case where an application has managed to build if this is not satisfied, but the compilation errors are really nasty and hard to make sense from.

I.e. Wen you build either application or kernel, make sure you have TinKer's headerfiles coming first in the search-path.

\paragraph{Assembly language}
Assembly language is a particularly difficult area, since it for natural resons can't be standardised as much as a high abstraction language like C.

\begin{itemize}
	\item Different CPU's use different instructions. The dissimilarities are big.
	\item Assembler notation can differ. For example GNU tool-chain commonly use the AT\&T notation. A very powerfull assembly notation, but a bit awkward to use.
	\item TinKer has grouped all it's architecture dependant code in in-line assembly \textit{macros}. Notation for in-line assembly differ between C-compilers.
\end{itemize}
This issue is very hard to address simply by writing the code in one way or the other. The only reasonable way to address it in my opinion, it is to keep the number of lines as low as possible. I.e. make use of the C-compiler as much as possible.

This has led to certain compromises in the kernel, where compile-time set-up has been replaced by run-time set-up. Some few $uS$ are lost, but the trade-off is fair.


\chapter{Standard library transparency}
This is an area of great importance, yet subtle enough to be easily missed as an issue at all.

When I first started out writing TinKer I thought I could get away without any standard library usage at all. In the \textit{"deeply embedded school"} there is a certain dislike against the usage of a standard library, more or less justified.

The reason why, is that it is believed that one looses control over the execution by not knowing what happens. Furthermore, there are certain issues concerning concurrency that are absolutely justifiable reasons \textit{not} to use a standart library\footnote{Or at least, not just any}.

The issue is tightly connected to the fact the traditional most embedded tool vendors didn't offer separable libraries and that the ones they did offer had certain flaws that would occasionally turn up as non-acceptable system behaviour.

The above issues are actually absolutely justifiable reasons, and to my knowledge only one other kernel\footnote{RTEMS} address this "by the book". The school-book way to handle this, is to rewrite or (adapt an existing) a standard library so that it satisfies all those time, behaviour and concurrency issues that you might have. A daunting task indeed\ldots

The issue with libc can be viewed from two angles
\begin{itemize}
	\item The TinKer kernel
	\item Your application
\end{itemize}
I've addressed the first point by letting the kernel have only two function calls it depends upon: \textit{clock()} and \textit{malloc()}. Both which can be handled even without the aid of a libc however.

The second angle is different. Except the API covered by POSIX 1003.1c and 1003.1b, the kernel doesn't aid the application in any way. I.e. you have to make sure youself if a certain function is safe to use in a concurrent environment or not. You can make it safe, it's not very hard. But the project domain has to be limited somewhere, and even if each API is relatively easy to certify, there are number of functions are just to be to address. Especially if you recall that we want to be as independent as possible, i.e. by being able to colaberate with more libc than one\ldots
The things to address when protecting your application
\begin{itemize}
	\item Reentrancy and thread safety\footnote{Functions in this category are all that uses file-handles. Most notably: printf, scanf\ldots}. If a function has \textit{"states"} or some other \textit{"side effect"}, wrap a mutex around it. Best thing to do is banning the usage of the function, and make the wrap in another following a certain naming convention thats easy for the programmer to remember. Optionally you can even wrap this new function around nifty macros, making appear as if it were the original function call\footnote{This is how TinKer does it}
	\item Dynamic memory allocation\footnote{alloc, malloc \'et all}. In an small embedded system you should carefully evaluate if you need dynamic memory allocation at all. You probably have limited resources, and even if concurrency is not the problem, starvation might be\footnote{And this is certainly not nice to find out during run-time}. Many alloc implementations also have an indeterministic execution time depending on if the available memory on the heap is fragmented or not. Dynamic memory allocation has states and is inheritly \textit{not thread safe}. For the GNU tool-chain this possible to handle because of the system function call-outs, and TinKer might address this issue differently in the future.
\end{itemize}
By originally having the ambition to not use any library at all, TinKer evolved to have very few dependencies towards one. Actually, TinKer is absolutely dependant of only one standard library function call to be able to dispatch a schedule\footnote{The primary purpose of any kernel}: \textit{clock()}. And even this is wrapped around a macro, so it can be temporarily dealt with\footnote{at least temporary when when you write a new port. A complete port must have this implemented}. However, no port is particularly usefull without being able to schedule real threads. So to complete a port, implementing following API will make life much easier to the programmer.
\begin{itemize}
	\item clock
	\item malloc
	\item printf
\end{itemize}
What about memory allocation then? You said this bas \textit{"a bad thing"}? It is, but only if used in a concurrent situation. TinKer needs memory for various of purposes, and using malloc\footnote{malloc and friends} is a convenient way to get it. However, we don't have to malloc and free all the time if we preallocate during startup. Instead we use preallocated memory\footnote{Which are sometimes indeed allocated by malloc and friends, but during start-up and before dispatching is started (i.e. when the CPU hasn't yet started to execute concurrent threads) } pools that store various kernel elements until they're needed. You only have to configure/initialise the kernel, and it will preallocate those pools during startup. 

This solves both the concurrency issue and the timeliness issue whit the small price of perhaps using slightly more RAM that the system otherwise \textit{on average}\footnote{Note that praxis and a very good rule of thumb regarding \textit{embedded} applications or \textit{real-time} applications, is to always \textit{\textbf{design by worst}} case (i.e. "on average" is a very bad thing to rely on) } would use otherwise.  However it doesn't solve the applications needs. One way to solve this could be to extend the internal service to cover the application also.

So to make wrap this discussion up a bit, issues with standard library can be dealt in several ways and by several instances. We concluded that whence you've built the kernel, it doesn't need any libc and that you application can handle the remaining issues as follows:

\begin{itemize}
	\item Thread safeting, using function wrappers and optionally additional macro wrappers
	\item Protection on system call-outs (GNU/Newlib only)
	\item Don't use libc.
\end{itemize}




\chapter{Real real-time}

\section{Preemption by need-only basis}
\section{Tinker's dual time concepts}

\chapter{Runnable in desktop as mini-kernels}
aiding of debugging and development

\chapter{Well defined limitations}
no filesys, no OS

\chapter{Source code organisation}
TinKer's source code is organised as in in table~\ref{sorce_tree}. The source in this tree is TinKer code only. Any examples, tutorials and preconfigured projects are not part of this tree. The TinKer project might make such trees available to the community, but it will in such case still not be part of the TinKer tree.
\\\\
The following subsections will explain the TinKer tree.
\begin{table}[!hbp]

\begin{verbatim}
tinker-x.y.x/
   tinker/
      src/
         *.c
         *.h
         arch/
            arm/
               *.h
            bfin/
               *.h
            powerpc/
               *.h
            :
            <CPUARCH>/
               *.h
      include/
         *.h
      lib/
         *.a
      bsp/
         gnu_arm/
            *.*
            /lp21xx
               *.*
         gnu_bfin/
            *.*
         gnu_powerpc/
            *.*
         keil_c166/
            *.*
         :
         <vendor>_<CPUARCH>
            *-*
            <BOARD>
               *.*

\end{verbatim}
\caption{TinKer source tree}\label{sorce_tree}
\end{table}
\section{\textit{tinker/src/} vs \textit{tinker/include/}}
To have a [project]/src directory instead of putting everything in the root-directory, is rather obvious and common. It makes distribution easier and it makes handling of several trees in the same CVS server manageable.

However, separating a \textit{[project]/include/} might not be as obvious.
\\\\
This is done for two reasons:
\begin{itemize}
	\item First of all it makes it very easy to install the headerfiles on the build system. Since we know where they are, and that the directory is not contaminated with intermediate build files, we just copy all of them.
	\item A program like a kernel is a fairly big project. As such we need to divide it in pieces like any normal project would. But because of a kernels nature, it has data that has to be visable by all the various parts of the kernel itself (i.e. all the kernel's files). We bind these togeather using headerfiles but we don't want \textit{those} headerfiles to be mistakenly used by the application programmer (thereby risking to corrupt the kernel in a very nasty way). These headerfiles are kept apart from the ones in \textit{tinker/include/} and are part of \textit{tinker/src/}. They are only used wen the kernel is build and not needed by any application.
\end{itemize}
\subsection{\textit{tinker/src/arch/}}
In this branch, \textit{generic} code relevant to various architectures is stored. Don't confuse this with the branch in \textit{tinker/bsp/}, where \textit{specific} code is stored.

\section{\textit{tinker/lib/}}
This is the directory where libtinker.a will be built but it's also an intermediate area. You'll need to link against libtinker.a from this directory in case you build the kernel with a non GNU tool-chain\footnote{When built and installed with a GNU tool-chain, both the build and source tree can be removed after installation.}.

TinKer will be build as a library. But during the process, it does so by building parts in smaller libraries. These libraries are to be considered as intermediate builds, but we need somewhere deductable to store them for the rules in various makefiles to be common and easy to write.

\section{\textit{tinker/bsp/}}
\textit{BSP} is na acronym commonly used as meaning \textit{"board support package"}. I.e. in this directory code for a specific \textit{"boards"} are stored. \textit{"Boards"} in turn not only imply specific peripherals, but also a certain main CPU architecture, tool-vendor specific stuff and in many cases also a specific CPU of each main family. 

The structure with it's adherent directory naming convention underneath \textit{tinker/bsp/} is there to handle those particular differences.
\\\\
The reason why we keep this tree apart from the other branches are as follows:
\begin{itemize}
	\item We need to limit the project somehow. BSP's represent a potentially very big code base since users are expected to make their own adaptions to fit their specific targets (i.e. BOARD).
	\item I.e. the \textit{tinker/bsp/} branch is a good place to draw that line, but still to provide some sort of "template" and interface to make it easy for those adaptions.
	\item This branch is expected to have a more relaxed policy regarding maintainers and contributors.
\end{itemize}

On this whole branch, maintainers are free to implement their port's as they like. They don't have to follow the same coding standard or use any particular files or particular filenames. The only rule is to name the topmost directory following the vendor\_arch convention, and to have one or more sub-directories where the actual code is stored. These subdirectories should be named such that the name implies which "board" it is intended for. If a BSP is (or has) a generic template, one of these sub-directories should be named \textit{"generic/"}.

What binds a BSP togeather with TinKer differs between tool-vendors. For GNU, which is the most common case and prefered choice, this is done by specific function calls that the BSP has to implement. Previously I memtioned the thre mandatory ones, but for a complete port one has to implement a few more\footnote{The functions are defined by GCC}.
 
\chapter{GNU and other thoughts}

